> **âš ï¸ Work in Progress**: This project is under active development. Core infrastructure is complete, but the neural network training pipeline and full Expert-Iteration loop are not yet implemented.

# FlowZero

FlowZero is a search-augmented reinforcement-learning agent, inspired by AlphaZero, designed to solve _Flow Free_ puzzles from first principles. The goal is to combine a hand-rolled Monte Carlo Graph Search with a lightweight ResNet policy-value network in an Expert-Iteration loop, while relying on established libraries only for tensor operations, logging, and continuous integration.

---

## Table of Contents

- [FlowZero](#flowzero)
  - [Table of Contents](#table-of-contents)
  - [Project Status](#project-status)
  - [Overview](#overview)
  - [Repository Layout](#repository-layout)
  - [Installation](#installation)
  - [What Works Now](#what-works-now)
  - [Roadmap](#roadmap)
  - [Acknowledgments \& References](#acknowledgments--references)
  - [License \& Disclaimer](#license--disclaimer)

---

## Project Status

### âœ… Implemented
- **Flow Free game engine** with complete board representation and move validation
- **SAT-based puzzle solver** for generating verified puzzle datasets
- **Monte Carlo Graph Search (MCGS)** with UCB1 selection and configurable rollouts
- **Puzzle generation pipeline** (handcrafted, synthetic, and unsolvable examples)
- **Configuration management** via YAML
- **CI/CD pipeline** with automated testing, linting, and formatting

### ğŸš§ In Progress
- **MCGS refinement and testing** (currently being validated and optimized)
- **Gymnasium environment** for RL integration

### ğŸ“‹ Not Yet Implemented
- **ResNet policy-value network** architecture
- **Expert-Iteration training loop** that combines MCGS with neural network learning
- **Self-play data generation** pipeline
- **Model checkpointing and evaluation** infrastructure

---

## Overview

Flow Free puzzles are cast as deterministic, episodic Markov Decision Processes (MDPs). The planned training will proceed in repeated Expert-Iteration cycles:

1. **Planning (Expert):**
   MCGS runs a fixed number of simulations per move, using the current ResNet's policy and value estimates.
2. **Learning (Apprentice):**
   A 6-block ResNet will be trained to
   - imitate the graph's move distribution (cross-entropy loss), and
   - predict final outcomes (mean-squared error loss).

This self-play framework will yield continual policy improvement without human-labeled data.

---

## Repository Layout

```plaintext
.
â”œâ”€â”€ flowzero_src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ handcrafted/       # Curated puzzle definitions
â”‚   â”‚   â”œâ”€â”€ synthetic/         # Automatically generated puzzles
â”‚   â”‚   â””â”€â”€ unsolvable/        # Negative examples (e.g. unsolvable_cross.txt)
â”‚   â”œâ”€â”€ flowfree/
â”‚   â”‚   â”œâ”€â”€ game.py            # âœ… Board representation, move validation
â”‚   â”‚   â””â”€â”€ solver.py          # âœ… SAT encoder & solver for data generation
â”‚   â”œâ”€â”€ gym/
â”‚   â”‚   â””â”€â”€ flowfree_gym_env.py # ğŸš§ Gymnasium environment wrapper
â”‚   â”œâ”€â”€ mcgs/
â”‚   â”‚   â””â”€â”€ mcgs.py            # ğŸš§ Monte Carlo Graph Search implementation
â”‚   â”œâ”€â”€ util/                  # âœ… Helper functions and utilities
â”‚   â”œâ”€â”€ generate_boards.py     # âœ… Procedural puzzle generator
â”‚   â””â”€â”€ train.py               # ğŸ“‹ Expert-Iteration training (not yet implemented)
â”œâ”€â”€ tests/                     # âœ… pytest suite with comprehensive coverage
â”‚   â”œâ”€â”€ test_game/
â”‚   â”œâ”€â”€ test_mcgs/
â”‚   â”œâ”€â”€ test_utilities/
â”‚   â””â”€â”€ conftest.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ config.yaml
â”œâ”€â”€ LICENSE
â””â”€â”€ .github/
    â””â”€â”€ workflows/ci.yml       # âœ… Linting, formatting, and test automation
```

---

## Installation

Create and activate a Python 3.10+ virtual environment (Python 3.13 recommended):

```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

Run tests to verify installation:

```bash
pytest
```

---

## What Works Now

You can currently:

- **Play Flow Free puzzles** using the game engine
- **Generate puzzles** of various difficulties
- **Solve puzzles** using the SAT-based solver to verify solvability
- **Run MCGS simulations** on puzzles (experimental, under testing)
- **Use the Gymnasium environment** for custom RL experiments

---

## Roadmap

The following components are planned for future development:

1. **Complete MCGS validation** - Finish testing and refining the Monte Carlo Graph Search implementation
2. **ResNet architecture** - Implement the 6-block ResNet for policy and value prediction
3. **Training pipeline** - Build the Expert-Iteration loop combining MCGS and neural network training
4. **Self-play system** - Create infrastructure for generating training data through self-play
5. **Evaluation framework** - Develop metrics and benchmarks to track agent improvement
6. **Model management** - Add checkpointing, versioning, and model comparison tools

---

## Acknowledgments & References

Expert Iteration: Anthony, Tian & Barber (2017) [Link](https://arxiv.org/abs/1705.08439)

AlphaZero: Silver et al. (2017) [Link](https://arxiv.org/abs/1712.01815)

Gymnasium: Towers et al. (2024) [Link](https://arxiv.org/abs/2407.17032)

Special Thanks: [Matt Zucker](https://github.com/mzucker), [Ben Torvaney](https://github.com/Torvaney), [Loki Chow](https://github.com/lohchness), and contributors

## License & Disclaimer
Apache 2.0 License. This project is not affiliated with Big Duck Games LLC or DeepMind.